{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Feature Extraction from Text\n",
    ">Dense vector features for text\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- image: https://static.wixstatic.com/media/e7e54e_e2bca71fdcb6452c9b1b80052799d199~mv2.png/v1/fill/w_560,h_366,al_c,q_85,usm_0.66_1.00_0.01/word2vec.webp\n",
    "- author: Uday Paila\n",
    "- categories: [NLP, feature extraction, word2vec, fasttext]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the [previous](https://udibhaskar.github.io/practical-ml/nlp/feature%20extraction/bow/tfidf/hashing%20vectorizer/2020/03/13/Basic-feature-Extraction.html) article, I discussed basic feature extraction methods like BOW, TFIDF  but, these are very sparse in nature. In this tutorial, we will try to explore word vectors this gives a dense vector for each word. There are many ways to get the dense vector representation for the words. below are some of them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co-occurrence Matrix and SVD\n",
    "We can create a co-occurrence matrix of text and then get a low rank approximation of matrix to get the dense feature representation.  \n",
    "\n",
    "To create a co-occurrence matrix, you go through text setting a window size around each word. You then keep track of which words appear in that window. \n",
    "\n",
    "lets create co-occurrence matrix with below sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_list = ['I like deeplearning.', 'I like NLP.', 'NLP is awesome.']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "with window size of 1. the co-occurrence matrix is \n",
    "\n",
    "<br>\n",
    "\n",
    "![coomatrix](https://i.imgur.com/tVfHiJg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`like` word came in context of `i` 2 times in window size one. in similar way, I updated above co-occurrence matrix with all counts. \n",
    "\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code\n",
    "\n",
    "I have written a brute force version of code below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "def cooccurrence_matrix(distance,sentances):\n",
    "    '''\n",
    "    Returns co-occurrence matrix of words with in a distance of occurrrence\n",
    "    input:\n",
    "    distance: distance between words(Window Size)\n",
    "    sentances: documets to check ( a list )\n",
    "    output:\n",
    "    co-occurance matrix in te order of list_words order\n",
    "    words list\n",
    "    '''\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "    tokenizer.fit_on_texts(sentances)\n",
    "    list_words = list(tokenizer.word_index.keys())\n",
    "    #print(list_words)\n",
    "    #length of matrix needed\n",
    "    l = len(list_words)\n",
    "    #creating a zero matrix\n",
    "    com = np.zeros((l,l))\n",
    "    #creating word and index dict\n",
    "    dict_idx = {v:i for i,v in enumerate(list_words)}\n",
    "    for sentence in sentances:\n",
    "        sentence = tokenizer.texts_to_sequences([sentence])[0]\n",
    "        tokens = [tokenizer.index_word[i] for i in sentence]\n",
    "        #tokens= sentence.split()\n",
    "        for pos,token in enumerate(tokens):\n",
    "            #if eord is in required words\n",
    "            if token in list_words:\n",
    "                #start index to check any other word occure or not\n",
    "                start=max(0,pos-distance)\n",
    "                #end index\n",
    "                end=min(len(tokens),pos+distance+1)\n",
    "                for pos2 in range(start,end):\n",
    "                    #if same position\n",
    "                    if pos2==pos:\n",
    "                        continue\n",
    "                    # if same word\n",
    "                    if token == tokens[pos2]:\n",
    "                        continue\n",
    "                    #if word found is in required words\n",
    "                    if tokens[pos2] in list_words:\n",
    "                        #index of word parent\n",
    "                        row = dict_idx[token]\n",
    "                        #index of occurance word\n",
    "                        col = dict_idx[tokens[pos2]]\n",
    "                        #adding value to that index\n",
    "                        com[row,col] = com[row,col] + 1\n",
    "    return com, list_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'like', 'nlp', 'deeplearning', 'is', 'awesome']\n",
      "[[0. 2. 0. 0. 0. 0.]\n",
      " [2. 0. 1. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "coo = cooccurrence_matrix(1, sent_list)\n",
    "print(coo[1])\n",
    "print(coo[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use SVD to get low rank approximation matrix(This will give dense matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.94649798e+00,  2.73880515e-15, -2.49727487e-01],\n",
       "       [-2.40633313e-15,  2.43040910e+00, -2.56144970e-01],\n",
       "       [ 1.20300191e+00,  1.58665133e-15,  4.04067562e-01],\n",
       "       [ 9.73248989e-01,  1.21830556e-15, -1.24863743e-01],\n",
       "       [ 7.73781546e-16,  5.73741760e-01,  1.08504750e+00],\n",
       "       [ 2.29752921e-01,  3.09635046e-16,  5.28931305e-01]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "tsvd = TruncatedSVD(n_components=3, n_iter=10, random_state=32 )\n",
    "dense_vector = tsvd.fit_transform(coo[0], )\n",
    "dense_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector of  ' like ' is  [-2.40633313e-15  2.43040910e+00 -2.56144970e-01]\n"
     ]
    }
   ],
   "source": [
    "print(\"Vector of \", \"'\" , coo[1][1], \"'\", \"is \", dense_vector[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "I think, there are many articles and videos regarding the Mathematics and Theory of Word2Vec. So, I am giving some links to explore and I will try to explain code to train the custom Word2Vec. Please check the resources below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">youtube: https://www.youtube.com/watch?list=PLUOY9Q6mTP21Al_odE-v_lmHDjVMSO9BX&v=SSpSk1Io52w&feature=emb_title\n",
    "\n",
    "<br>\n",
    "\n",
    "You can read a good blog [here](https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html)\n",
    "\n",
    "<br>\n",
    "\n",
    "Please watch the above videos or read the above blog before going into the coding part. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec using Gensim\n",
    "\n",
    "We can train `word2vec` using `gensim` module with `CBOW` or `Skip-Gram` ( Hierarchical Softmax/Negative Sampling). It is one of the efficient ways to train word vectors. I am training word vectors using gensim, using IMDB reviews as a data corpus to train. In this, I am not training the best word vectors, only training for 10 iterations.\n",
    "\n",
    "<br>\n",
    "\n",
    "To train `gensim word2vec` module, we can give a list of sentences or a file a corpus file in  `LineSentence` format. Here I am creating a list of sentences from my corpus. If you have huge data, please try to use `LineSentence` format to efficiently train your word vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##getting sentence wise data\n",
    "list_sents = [nltk.word_tokenize(sent) for sent_tok in data_imdb.review for sent in nltk.sent_tokenize(sent_tok)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Training `gensim word2vec` as below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##import gensim\n",
    "from gensim.models import Word2Vec\n",
    "##word2vec model ##this may take some time to execute. \n",
    "word2vec_model = Word2Vec(list_sents,##list of sentences, if you don;t have all the data in RAM, you can give file name to corpus_file \n",
    "                          size=50, ##output size of word emebedding \n",
    "                          window=4, ##window size\n",
    "                          min_count=1, ## ignors all the words with total frquency lower than this\n",
    "                          workers=5, ##number of workers to use\n",
    "                          sg=1, ## skip gram\n",
    "                          hs=0, ## 1 --> hierarchical, 0 --> Negative sampling\n",
    "                          negative=5, ##How many negative samples\n",
    "                          alpha=0.03, ##The initial learning rate\n",
    "                          min_alpha=0.0001, ##Learning rate will linearly drop to min_alpha as training progresses.\n",
    "                          seed = 54, ##random seed\n",
    "                          iter=10,\n",
    "                         compute_loss=True)##number of iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get word vectors as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##getting a word vector\n",
    "word2vec_model.wv['movie']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get most similar positive words for any given word as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##getting most similar positive words\n",
    "word2vec_model.wv.most_similar(positive='movie')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can save your model as below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##saving the model\n",
    "word2vec_model.save('w2vmodel/w2vmodel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get the total notebook in the below GitHub link\n",
    "\n",
    ">github: https://github.com/UdiBhaskar/Natural-Language-Processing/blob/master/Feature%20Extraction%20Methods/Advanced%20feature%20extraction%20-%20W2V/W2V_using_Gensim.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec using Tensorflow ( Skip-Gram, Negative Sampling)\n",
    "\n",
    "In the negative sampling, we will get a positive pair of `skip-grams` and for every positive pair, we will generate n number of negative pairs. I used only 10 negative pairs. In the paper, they suggesting around 25. Now we will use these positive and negative pairs and try to create a classifier that differentiates both positive and negative samples. While doing this, we will learn the word vectors.\n",
    "We have to train a classifier that differentiates positive sample and negative samples, while doing this we will learn the word embedding. Classifier looks like below image \n",
    "\n",
    "<br>\n",
    "\n",
    "![kerasNS](https://i.imgur.com/4Nt9nNF.png \"W2V using NS\")\n",
    "\n",
    "<br>\n",
    "\n",
    "The above model takes two inputs center word, context word and, model output is one if those two words occur within a window size else zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the data\n",
    "We have to generate the `skip-gram` pairs and negative samples. We can do that easily using `tf.keras.preprocessing.sequence.skipgrams`.  This also takes a probability table(sampling table), in which we can give the probability of that word to utilize in the negative samples i.e. we can make probability low for the most frequent words and high probability for the least frequent words while generating negative samples.  \n",
    "\n",
    "Converted total words into the number sequence. Numbers are given in descending order of frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##to use tf.keras.preprocessing.sequence.skipgrams, we have to encode our sentence to numbers. so used Tokenizer class\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(list_sents)\n",
    "seq_texts = tokenizer.texts_to_sequences(list_sents) ##list of list+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we create total samples at once, it may take so much `RAM` and that gives the resource exhaust error. so created a generator function which generates the values batchwise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Skipgram with Negativive sampling generator \n",
    "##for generating the skip gram negative samples we can use tf.keras.preprocessing.sequence.skipgrams and \n",
    "#internally uses sampling table so we need to generate sampling table with tf.keras.preprocessing.sequence.make_sampling_table\n",
    "sampling_table_ns = tf.keras.preprocessing.sequence.make_sampling_table(size=len(tokenizer.word_index)+1,   \n",
    "                                                                        sampling_factor=1e-05)\n",
    "def generate_sgns():\n",
    "    ##loop through all the sequences\n",
    "    for seq in seq_texts:\n",
    "        generated_samples, labels = tf.keras.preprocessing.sequence.skipgrams(sequence=seq, \n",
    "                                                                      vocabulary_size=len(tokenizer.word_index)+1, \n",
    "                                                                      window_size=3, negative_samples=10, \n",
    "                                                                      sampling_table=sampling_table_ns)\n",
    "        length_samples = len(generated_samples)\n",
    "        for i in range(length_samples):\n",
    "            ##centerword, context word, label\n",
    "            yield [generated_samples[i][0]], [generated_samples[i][1]], [labels[i]]\n",
    "\n",
    "##creating the tf dataset\n",
    "tfdataset_gen = tf.data.Dataset.from_generator(generate_sgns, output_types=(tf.int64, tf.int64, tf.int64))\n",
    "tfdataset_gen = tfdataset_gen.repeat().batch(2048).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##fixing numpy RS\n",
    "np.random.seed(42)\n",
    "\n",
    "##fixing tensorflow RS\n",
    "tf.random.set_seed(32)\n",
    "\n",
    "##python RS\n",
    "rn.seed(12)\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "##model\n",
    "def getSGNS():\n",
    "    \n",
    "    center_word_input= Input(shape=(1,), name=\"center_word_input\")\n",
    "    context_word_input= Input(shape=(1,), name=\"context_word_input\")\n",
    "    \n",
    "    ##i am initilizing randomly. But you can use predefined embeddings. \n",
    "    embedd_layer = Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=100,\n",
    "                    embeddings_initializer=tf.keras.initializers.RandomUniform(seed=45),\n",
    "                     name=\"Embedding_layer\")\n",
    "    \n",
    "    #center word embedding\n",
    "    center_wv = embedd_layer(center_word_input)\n",
    "    \n",
    "    #context word embedding\n",
    "    context_wv = embedd_layer(context_word_input)\n",
    "    \n",
    "    #dot product\n",
    "    dot_out = Dot(axes=2, name=\"dot_between_center_context\")([center_wv, context_wv]) \n",
    "    \n",
    "    dot_out = Reshape((1,), name=\"reshaping\")(dot_out)\n",
    "    \n",
    "    final_out = Dense(1, activation='sigmoid', kernel_initializer=tf.keras.initializers.glorot_uniform(seed=54),\n",
    "                  name=\"output_layer\")(dot_out)\n",
    "    \n",
    "    basic_w2v = Model(inputs=[center_word_input, context_word_input], outputs=final_out, name=\"sgns_w2v\")\n",
    "    \n",
    "    return basic_w2v\n",
    "\n",
    "\n",
    "sgns_w2v = getSGNS()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##training\n",
    "\n",
    "##optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.005)\n",
    "\n",
    "##train step function to train\n",
    "@tf.function\n",
    "def train_step(input_center, input_context, output_vector, loss_fn):\n",
    "    with tf.GradientTape() as tape:\n",
    "        #forward propagation\n",
    "        output_predicted = sgns_w2v(inputs=[input_center, input_context], training=True)\n",
    "        #loss\n",
    "        loss = loss_fn(output_vector, output_predicted)\n",
    "    #getting gradients\n",
    "    gradients = tape.gradient(loss, sgns_w2v.trainable_variables)\n",
    "    #applying gradients\n",
    "    optimizer.apply_gradients(zip(gradients, sgns_w2v.trainable_variables))\n",
    "    return loss, gradients\n",
    "\n",
    "##number of epochs\n",
    "no_iterations=100000\n",
    "\n",
    "##metrics # Even if you use .fit method, it alsocalculates batchwise loss/metric and aggregates those.  \n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "\n",
    "#tensorboard file writers\n",
    "wtrain = tf.summary.create_file_writer(logdir='/content/drive/My Drive/word2vec/logs/w2vns/train')\n",
    "\n",
    "##creating a loss object for this classification problem\n",
    "loss_function = tf.keras.losses.BinaryCrossentropy(from_logits=False, \n",
    "                                                                reduction='auto')\n",
    "\n",
    "##check point to save\n",
    "checkpoint_path = \"/content/drive/My Drive/word2vec/checkpoints/w2vNS/train\"\n",
    "ckpt = tf.train.Checkpoint(optimizer=optimizer, model=sgns_w2v)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=3)\n",
    "\n",
    "counter = 0\n",
    "#training anf validating\n",
    "for in_center, in_context, out_label in tfdataset_gen:\n",
    "    #train step\n",
    "    loss_, gradients = train_step(in_center, in_context, out_label, loss_function)\n",
    "    #adding loss to train loss\n",
    "    train_loss(loss_)\n",
    "\n",
    "    counter = counter + 1\n",
    "          \n",
    "    ##tensorboard \n",
    "    with tf.name_scope('per_step_training'):\n",
    "        with wtrain.as_default():\n",
    "            tf.summary.scalar(\"batch_loss\", loss_, step=counter)\n",
    "    with tf.name_scope(\"per_batch_gradients\"):\n",
    "        with wtrain.as_default():\n",
    "            for i in range(len(sgns_w2v.trainable_variables)):\n",
    "                name_temp = sgns_w2v.trainable_variables[i].name\n",
    "                tf.summary.histogram(name_temp, gradients[i], step=counter)\n",
    "    \n",
    "    if counter%100 == 0:\n",
    "        #printing\n",
    "        template = '''Done {} iterations, Loss: {:0.6f}'''\n",
    "    \n",
    "        print(template.format(counter, train_loss.result()))\n",
    "\n",
    "        if counter%200 == 0:\n",
    "            ckpt_save_path  = ckpt_manager.save()\n",
    "            print ('Saving checkpoint for iteration {} at {}'.format(counter+1, ckpt_save_path))\n",
    "        \n",
    "        train_loss.reset_states()\n",
    "    if counter > no_iterations:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check total code and results in my GitHub link below.\n",
    "\n",
    ">github: https://github.com/UdiBhaskar/Natural-Language-Processing/blob/master/Feature%20Extraction%20Methods/Advanced%20feature%20extraction%20-%20W2V/W2V_Tensorflow_Negative_Sampling.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saved the model into gensim Word2Vec format and loaded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_word2vec_format_dict(binary=True, fname='w2vns.bin', total_vec=len(word_vectors_dict), vocab=model_gensim.vocab, vectors=model_gensim.vectors)\n",
    "model_gensim = gensim.models.keyedvectors.Word2VecKeyedVectors.load_word2vec_format('w2vns.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Important: `Negative Sampling` is a simplified version of `Noise Contrastive Estimation`. `NCE` guarantees approximation to softmax, `Negative Sampling` doesnâ€™t. You can read this in [paper](https://arxiv.org/abs/1410.8251)/[blog](https://ruder.io/word-embeddings-softmax/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec using Tensorflow (Skip-Gram, NCE)\n",
    "Let's take a which gives the score to each pair of the `skip-grams`, we will try to maximize the `(score of positive pairs to the word - score of negative pairs)` to the word. We can do that directly by optimizing the `tf.nn.nce_loss`. Please try to read the documentation. It takes a positive pair, weight vectors and then generates the negative pairs based on `sampled_values`, and gives the loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the Data\n",
    "We have to generate a positive pair of skip-grams, we can do it in a similar way as above. Created a pipeline to generate batchwise data as below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##getting sentence wise data\n",
    "list_sents = [nltk.word_tokenize(sent) for sent_tok in data_imdb.review for sent in nltk.sent_tokenize(sent_tok)]\n",
    "##to use tf.keras.preprocessing.sequence.skipgrams, we have to encode our sentence to numbers. so used Tokenizer class\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(list_sents)\n",
    "seq_texts = tokenizer.texts_to_sequences(list_sents) ##list of list\n",
    "\n",
    "def generate_sgns():\n",
    "    for seq in seq_texts:\n",
    "        generated_samples, labels = tf.keras.preprocessing.sequence.skipgrams(sequence=seq, \n",
    "                                                                      vocabulary_size=len(tokenizer.word_index)+1, \n",
    "                                                                      window_size=2, negative_samples=0)\n",
    "        length_samples = len(generated_samples)\n",
    "        for i in range(length_samples):\n",
    "            yield [generated_samples[i][0]], [generated_samples[i][1]]\n",
    "\n",
    "##creating the tf dataset\n",
    "tfdataset_gen = tf.data.Dataset.from_generator(generate_sgns, output_types=(tf.int64, tf.int64))\n",
    "tfdataset_gen = tfdataset_gen.repeat().batch(1024).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Model\n",
    "I created a model word2vecNCS which takes a center word, context word and give NCE loss. You can check that below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class word2vecNCS(Model):\n",
    "    def __init__(self, vocab_size, embed_size, num_sampled, **kwargs):\n",
    "        '''NCS Word2Vec\n",
    "        vocab_size: Size of vocabulary you have\n",
    "        embed_size: Embedding size needed\n",
    "        num_sampled: No of negative sampled to generate'''\n",
    "        super(word2vecNCS, self).__init__(**kwargs)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.num_sampled = num_sampled\n",
    "        ##embedding layer\n",
    "        self.embed_layer = Embedding(input_dim=vocab_size, output_dim=embed_size,embeddings_initializer=tf.keras.initializers.RandomUniform(seed=32))\n",
    "        ##reshing layer\n",
    "        self.reshape_layer = Reshape((self.embed_size,))\n",
    "    def build(self, input_shape):\n",
    "        ##weights needed for nce loss\n",
    "        self.nce_weight = self.add_weight(shape=(self.vocab_size, self.embed_size),\n",
    "                             initializer=tf.keras.initializers.TruncatedNormal(mean=0, stddev= (1/self.embed_size**0.5)),\n",
    "                             trainable=True, name=\"nce_weight\")\n",
    "        #biases needed nce loss\n",
    "        self.nce_bias = self.add_weight(shape=(self.vocab_size), initializer=\"zeros\", trainable=True, name=\"nce_bias\")\n",
    "\n",
    "    def call(self, input_center_word, input_context_word):\n",
    "        '''\n",
    "        input_center_word: center word\n",
    "        input_context_word: context word''' \n",
    "        ##giving center word and getting the embedding\n",
    "        embedd_out = self.embed_layer(input_center_word)\n",
    "        ##rehaping \n",
    "        embedd_out = self.reshape_layer(embedd_out)\n",
    "        ##calculating nce loss\n",
    "        nce_loss = tf.reduce_sum(tf.nn.nce_loss(weights=self.nce_weight, \n",
    "                                  biases=self.nce_bias, \n",
    "                                  labels=input_context_word, \n",
    "                                  inputs=embedd_out, \n",
    "                                  num_sampled=self.num_sampled, \n",
    "                                  num_classes=self.vocab_size))\n",
    "        return nce_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##training\n",
    "\n",
    "##optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.005)\n",
    "\n",
    "sgncs_w2v = word2vecNCS(len(tokenizer.word_index)+1, 100, 32, name=\"w2vNCE\")\n",
    "\n",
    "##train step function to train\n",
    "@tf.function\n",
    "def train_step(input_center, input_context):\n",
    "    with tf.GradientTape() as tape:\n",
    "        #forward propagation\n",
    "        loss = sgncs_w2v(input_center, input_context)\n",
    "\n",
    "    #getting gradients\n",
    "    gradients = tape.gradient(loss, sgncs_w2v.trainable_variables)\n",
    "    #applying gradients\n",
    "    optimizer.apply_gradients(zip(gradients, sgncs_w2v.trainable_variables))\n",
    "\n",
    "    return loss, gradients\n",
    "\n",
    "##number of epochs\n",
    "no_iterations=10000\n",
    "\n",
    "##metrics # Even if you use .fit method, it alsocalculates batchwise loss/metric and aggregates those.  \n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "\n",
    "#tensorboard file writers\n",
    "wtrain = tf.summary.create_file_writer(logdir='/content/drive/My Drive/word2vec/logs/w2vncs/train')\n",
    "\n",
    "##check point to save\n",
    "checkpoint_path = \"/content/drive/My Drive/word2vec/checkpoints/w2vNCS/train\"\n",
    "ckpt = tf.train.Checkpoint(optimizer=optimizer, model=sgncs_w2v)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=3)\n",
    "\n",
    "\n",
    "counter = 0\n",
    "#training anf validating\n",
    "for in_center, in_context in tfdataset_gen:\n",
    "    #train step\n",
    "    loss_, gradients = train_step(in_center, in_context)\n",
    "    #adding loss to train loss\n",
    "    train_loss(loss_)\n",
    "\n",
    "    counter = counter + 1\n",
    "         \n",
    "    ##tensorboard \n",
    "    with tf.name_scope('per_step_training'):\n",
    "        with wtrain.as_default():\n",
    "            tf.summary.scalar(\"batch_loss\", loss_, step=counter)\n",
    "    with tf.name_scope(\"per_batch_gradients\"):\n",
    "        with wtrain.as_default():\n",
    "            for i in range(len(sgncs_w2v.trainable_variables)):\n",
    "                name_temp = sgncs_w2v.trainable_variables[i].name\n",
    "                tf.summary.histogram(name_temp, gradients[i], step=counter)\n",
    "    \n",
    "    if counter%100 == 0:\n",
    "        #printing\n",
    "        template = '''Done {} iterations, Loss: {:0.6f}'''\n",
    "    \n",
    "        print(template.format(counter, train_loss.result()))\n",
    "\n",
    "        if counter%200 == 0:\n",
    "            ckpt_save_path  = ckpt_manager.save()\n",
    "            print ('Saving checkpoint for iteration {} at {}'.format(counter+1, ckpt_save_path))\n",
    "        \n",
    "        train_loss.reset_states()\n",
    "    if counter > no_iterations :\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check total code and results in my GitHub link below. \n",
    "\n",
    ">github: https://github.com/UdiBhaskar/Natural-Language-Processing/blob/master/Feature%20Extraction%20Methods/Advanced%20feature%20extraction%20-%20W2V/W2V_Tensorflow_NCE.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fast-text Embedding (Sub-Word Embedding)\n",
    "\n",
    "Instead of feeding individual words into the Neural Network, `FastText` breaks words into several n-grams (sub-words). For instance, tri-grams for the word where is `<wh, whe, her, ere, re>` and the special sequence `<where>`. Note that the sequence, corresponding to the word her is different from the tri-gram her from the word where. Because of these subwords, we can get embedding for any word we have even it is a misspelled word. Try to read [this](https://arxiv.org/pdf/1607.04606.pdf) paper.\n",
    "\n",
    "<br>\n",
    "\n",
    "We can train these vectors using the `gensim` or `fastText` official implementation. Trained `fastText` word embedding with gensim, you can check that below.  It's a single line of code similar to Word2vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##FastText module\n",
    "from gensim.models import FastText\n",
    "gensim_fasttext = FastText(sentences=list_sents, \n",
    "                           sg=1, ##skipgram\n",
    "                           hs=0, #negative sampling \n",
    "                           min_count=4, ##min count of any vocab \n",
    "                           negative=10, ##no of negative samples \n",
    "                           iter=15, ##no of iterations\n",
    "                           size=100, ##dimentions of word\n",
    "                           window=3, ##window size to get the skipgrams\n",
    "                           seed=34)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get the total code in the below GitHub\n",
    "\n",
    ">github: https://github.com/UdiBhaskar/Natural-Language-Processing/blob/master/Feature%20Extraction%20Methods/Advanced%20feature%20extraction%20-%20W2V/fasttext_Training.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Trained Word Embedding\n",
    "We can get pre-trained word embedding that was trained on huge data by Google, Stanford NLP, Facebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Word2Vec\n",
    "You can download google's pretrained wordvectors trained on Google news data from [this](https://code.google.com/archive/p/word2vec/) link. You can load the vectors as `gensim` model like below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "googlew2v_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe Pretrained Embeddings\n",
    "You can download the glove embedding from [this](https://nlp.stanford.edu/projects/glove/) link. There are some differences between Google Word2vec save format and GloVe save format. We can convert Glove format to google format and then load that using `gensim` as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove2word2vec(glove_input_file=\"glove.42B.300d.txt\", word2vec_output_file=\"w2vstyle_glove_vectors.txt\")\n",
    "\n",
    "glove_model = gensim.models.KeyedVectors.load_word2vec_format(\"w2vstyle_glove_vectors.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastText Pretrained Embeddings\n",
    "You can get the `fasttext` word embeedings from [this](https://fasttext.cc/docs/en/crawl-vectors.html) link. You can use `fasttext` python api or `gensim` to load the model. I am using gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "fasttext_model = FastText.load_fasttext_format(\"/content/cc.en.300.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References: \n",
    "1. gensim documentation\n",
    "2. https://fasttext.cc/\n",
    "3. CS7015 - IIT Madras\n",
    "4. https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html\n",
    "5. https://arxiv.org/abs/1410.8251\n",
    "6. https://ruder.io/word-embeddings-softmax/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
