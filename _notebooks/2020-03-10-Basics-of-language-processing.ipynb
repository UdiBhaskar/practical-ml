{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics of Natural Language Processing\n",
    ">Natural language processing basics and pipeline\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- image:https://i.imgur.com/xVLZ7Uk.png\n",
    "- author: Uday Paila\n",
    "- categories: [NLP, Basics, pipeline, text cleaning, Tokenization]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In any language, below are some language analysis categories.  I will try to write basic processing using spaCy and NLTK.\n",
    "\n",
    "<br>\n",
    "\n",
    "![Basics of NLP](https://i.imgur.com/SrpkOud.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexical Analysis\n",
    "Lexical analysis is the task of segmenting text into its lexical expressions i.e. words/tokens.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Tokenization\n",
    "Converting sentence into tokens/words called as `tokenization`. There are many ways to do this. I will discuss some of them below.  I am also creating 3 sentences as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"@uday can't wait for the #nlp notes YAAAAAAY!!! #deeplearning https://udibhaskar.github.io/practical-ml/\", 'That U.S.A. poster-print costs $12.40...', 'I am writing NLP basics.']\n"
     ]
    }
   ],
   "source": [
    "demo_sent1 = \"@uday can't wait for the #nlp notes YAAAAAAY!!! #deeplearning https://udibhaskar.github.io/practical-ml/\"\n",
    "demo_sent2 = \"That U.S.A. poster-print costs $12.40...\"\n",
    "demo_sent3 = \"I am writing NLP basics.\"\n",
    "all_sents = [demo_sent1, demo_sent2, demo_sent3]\n",
    "print(all_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### White Space Tokenizer\n",
    "We can tokenize the data by splitting the data at space. check the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@uday', \"can't\", 'wait', 'for', 'the', '#nlp', 'notes', 'YAAAAAAY!!!', '#deeplearning', 'https://udibhaskar.github.io/practical-ml/']\n",
      "['That', 'U.S.A.', 'poster-print', 'costs', '$12.40...']\n",
      "['I', 'am', 'writing', 'NLP', 'basics.']\n"
     ]
    }
   ],
   "source": [
    "for sent in all_sents:\n",
    "    print(sent.split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some of the words, it is working perfectly like `U.S.A.`, `poster-printer` but we are getting `@uday`, `basics.`, `$12.40...`, `#nlp` as words but we have to remove those `#`,`@`,`.` etc... So this tokenizer may give bad results if we have words like this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK Word Tokenizer\n",
    "It follows the conventions of the `Penn Treebank`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@', 'uday', 'ca', \"n't\", 'wait', 'for', 'the', '#', 'nlp', 'notes', 'YAAAAAAY', '!', '!', '!', '#', 'deeplearning', 'https', ':', '//udibhaskar.github.io/practical-ml/']\n",
      "['That', 'U.S.A.', 'poster-print', 'costs', '$', '12.40', '...']\n",
      "['I', 'am', 'writing', 'NLP', 'basics', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "for sent in all_sents:\n",
    "    print(word_tokenize(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is giving better results compared to the white space tokenizer but some words like `can't` and `web addresses` are not working fine. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK Regex Tokenizer\n",
    "We can write our own regex to split the sentence into tokens/words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@', 'uday', 'can', \"'\", 't', 'wait', 'for', 'the', 'nlp', 'notes', 'YAAAAAAY', 'deeplearning', 'https', ':', 'udibhaskar', '.', 'github', '.', 'io', 'practical-ml']\n",
      "['That', 'U.S.A.', 'poster-print', 'costs', '$12.40', '...']\n",
      "['I', 'am', 'writing', 'NLP', 'basics', '.']\n"
     ]
    }
   ],
   "source": [
    "pattern = r'''(?x)     # set flag to allow verbose regexps\n",
    "...     (?:[A-Z]\\.)+       # abbreviations, \n",
    "...   | \\w+(?:-\\w+)*       # words with optional internal hyphens\n",
    "...   | \\$?\\d+(?:\\.\\d+)?%? # currency and percentages, \n",
    "...   | \\.\\.\\.             # ellipsis\n",
    "...   | [][.,;\"'?():-_`]   # these are separate tokens; includes ], [\n",
    " '''\n",
    "for sent in all_sents:\n",
    "    print(nltk.regexp_tokenize(sent, pattern))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Note: There are many more NLTK tokenizers. You can refer to all of them in [this](https://www.nltk.org/api/nltk.tokenize.html) link."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### spaCy Tokenizer\n",
    "Works on predefined regular expression rules for prefix_search, suffix_search, infix_finditer, token_match, and also Dependency Parsing to find sentence boundaries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@uday', 'ca', \"n't\", 'wait', 'for', 'the', '#', 'nlp', 'notes', 'YAAAAAAY', '!', '!', '!', '#', 'deeplearning', 'https://udibhaskar.github.io/practical-ml/']\n",
      "['That', 'U.S.A.', 'poster', '-', 'print', 'costs', '$', '12.40', '...']\n",
      "['I', 'am', 'writing', 'NLP', 'basics', '.']\n"
     ]
    }
   ],
   "source": [
    "##loading spaCy english module\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "#printing\n",
    "for sent in all_sents:\n",
    "    print([token.text for token in nlp(sent)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some tokenizers like `SentencePiece` that can learn how to `tokenize` form corpus of the data. I will discuss this in another blog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Warning: Our analysis and model performance also depends on the `Tokenization` algorithm so be careful while choosing the tokenization algorithm. If possible try with two or more algorithms or try to write custom rules based on your dataset/task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Morphological Analysis\n",
    "In linguistics, morphology is the study of the internal structure of words. I will try to explain some of them below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "Using morphological analysis to return the dictionary form of a word i.e. the entry in a dictionary you would find all forms under.  In Lemmatization root word is called Lemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\n",
      "runner\n",
      "runner\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize('running'))\n",
    "print(lemmatizer.lemmatize('runner'))\n",
    "print(lemmatizer.lemmatize('runners'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "Stemming is the process of producing morphological variants of a root/base word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "runner\n",
      "runner\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "print(stemmer.stem('running'))\n",
    "print(stemmer.stem('runner'))\n",
    "print(stemmer.stem('runners'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will try to explain some other pre-processing techniques like `POS tagging`, `Dependency Parsing` while doing deep learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Words\n",
    "stop words usually refers to the most common words in a language, there is no single universal list of stop words used by all natural language processing tools, and indeed not all tools even use such a list. We  can remove the stop words if you don't need exact meaning of a sentence. For text classification, we don't need those most of the time but, we need those for question and answer systems. word `not` is also a stop word in `NLTK` and this may be useful while classifying positive/negative sentence so be careful while removing the `stopwords`. You can get the stop words from `NLTK` as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP pipeline\n",
    "![nlp pipeline](https://i.imgur.com/VQ6FYoF.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Preprocessing\n",
    "You may get data from PDF files, speech, OCR, Docs, Web so you have to preprocess the data to get the better raw text. I would recommend you to read [this](https://www.kdnuggets.com/2018/03/text-data-preprocessing-walkthrough-python.html) blog. \n",
    "\n",
    "<br>\n",
    "\n",
    "![text processing](https://i.imgur.com/xVLZ7Uk.png)\n",
    "\n",
    "<br>\n",
    "Once you are done with basic cleaning, I would suggest do everything with `spaCy`. It is very easy to write the total pipeline. I took the `IMDB` dataset and written a pipeline to clean the data and get the tokens/words from the data. Before going to that, please check the below notebook that explains spaCy.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "I have written a class `TextPreprocess` which takes a raw text and gives tokens which will be given to the ML/DL algorithm. It will be very useful while deploying the algorithm in production if we write a clear pipeline like below. Writing this may take several days of analysis on the real-life text data. Once you have done with total analysis, please try to write a structured function/class which takes raw data and gives data that will be fed to the algorithm or another preprocessing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check below link to know more about pipeline\n",
    "class TextPreprocess():\n",
    "    def __init__(self):\n",
    "        ##loading nlp object of spacy\n",
    "        self.nlp = spacy.load(\"en_core_web_lg\", disable=[\"tagger\", \"parser\"])\n",
    "        # adding it to nlp object\n",
    "        self.merge_entities_ = self.nlp.create_pipe(\"merge_entities\")\n",
    "        self.nlp.add_pipe(self.merge_entities_)\n",
    "        \n",
    "        ##removing not, neitherm never from stopwords,\n",
    "        ##you can check all the spaCy stopwords from https://github.com/explosion/spaCy/blob/master/spacy/lang/en/stop_words.py\n",
    "        self.nlp.vocab[\"not\"].is_stop = False\n",
    "        self.nlp.vocab['neither'].is_stop = False\n",
    "        self.nlp.vocab['never'].is_stop = False\n",
    "        \n",
    "    def clean_raw_text(self, text, remove_html=True, clean_dots=True, clean_quotes=True, \n",
    "               clean_whitespace=True, convert_lowercase=True):\n",
    "        \"\"\"\n",
    "        Clean the text data.\n",
    "        text: input raw text data\n",
    "        remove_html: if True, it removes the HTML tags and gives the only text data. \n",
    "        clean_dots: cleans all type of dots to fixed one\n",
    "        clean_quotes: changes all type of quotes to fixed type like \"\n",
    "        clean_whitespaces: removes 2 or more white spaces\n",
    "        convert_lowercase: converts text to lower case\n",
    "        \"\"\"\n",
    "        if remove_html:\n",
    "            # remove HTML\n",
    "            ##separator=' ' to replace tags with space. othewise, we are getting some unwanted type like\n",
    "            ## \"make these characters come alive.<br /><br />We wish\" --> make these characters come alive.We wish (no space between sentences)\n",
    "            text = BeautifulSoup(text, 'html.parser').get_text(separator=' ')  \n",
    "            \n",
    "        # https://github.com/blendle/research-summarization/blob/master/enrichers/cleaner.py#L29\n",
    "        if clean_dots:\n",
    "            text = re.sub(r'…', '...', text)\n",
    "        if clean_quotes:\n",
    "            text = re.sub(r'[`‘’‛⸂⸃⸌⸍⸜⸝]', \"'\", text)\n",
    "            text = re.sub(r'[„“]|(\\'\\')|(,,)', '\"', text)\n",
    "            text = re.sub(r'[-_]', \" \", text)\n",
    "        if clean_whitespace:\n",
    "            text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        if convert_lowercase:\n",
    "            text = text.lower()\n",
    "        return text\n",
    "    \n",
    "    def get_token_list(self, text, get_spacy_tokens=False):\n",
    "        '''\n",
    "        gives the list of spacy tokens/word strings\n",
    "        text: cleaned text\n",
    "        get_spacy_tokens: if true, it returns the list of spacy token objects\n",
    "                          else, returns tokens in string format\n",
    "        '''\n",
    "        ##nlp object\n",
    "        doc = self.nlp(text)\n",
    "        out_tokens = []\n",
    "        for token in doc:\n",
    "            if token.ent_type_ == \"\":\n",
    "                if not(token.is_punct or token.is_stop):\n",
    "                    if get_spacy_tokens:\n",
    "                        out_tokens.append(token)\n",
    "                    else:\n",
    "                        out_tokens.append(token.norm_)\n",
    "        return out_tokens\n",
    "    \n",
    "    def get_preprocessed_tokens(self, text, remove_html=True, clean_dots=True, clean_quotes=True, \n",
    "               clean_whitespace=True, convert_lowercase=True, get_tokens=True, get_spacy_tokens=False, get_string=False):\n",
    "        \"\"\"\n",
    "        returns the cleaned text\n",
    "        text: input raw text data\n",
    "        remove_html: if True, it removes the HTML tags and gives the only text data. \n",
    "        clean_dots: cleans all type of dots to fixed one\n",
    "        clean_quotes: changes all type of quotes to fixed type like \"\n",
    "        clean_whitespaces: removes 2 or more white spaces\n",
    "        convert_lowercase: converts text to lower case\n",
    "        get_tokens: if true, returns output after tokenization else after cleaning only.\n",
    "        get_spacy_tokens: if true, it returns the list of spacy token objects\n",
    "                          else, returns tokens in string format\n",
    "        get_string: returns string output(combining all tokens by space separation) only if get_spacy_tokens=False\n",
    "        \"\"\"\n",
    "        text = self.clean_raw_text(text, remove_html, clean_dots, clean_quotes, clean_whitespace, convert_lowercase)\n",
    "        if get_tokens:\n",
    "            text = self.get_token_list(text, get_spacy_tokens)\n",
    "            if (get_string and (not get_spacy_tokens)):\n",
    "                text = \" \".join(text)\n",
    "        return text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
