{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Feature Extraction Methods\n",
    ">Feature Extraction from raw text\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- image:https://miro.medium.com/max/1400/1*ns1YWa4cU78B2bpkSOky7w.png\n",
    "- author: Uday Paila\n",
    "- categories: [NLP, feature extraction, bow, tfidf, hashing vectorizer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Term Matrix\n",
    "It is a matrix with rows contains unique documents and the column contain the unique words/tokens. Let's take sample documents and store them in the `sample_documents`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_documents = ['This is the NLP notebook', \n",
    "                    'This is basic NLP. NLP is easy',\n",
    "                    'NLP is awesome']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above sample_documents, we have 3 documents and 8 unique words. The Document Term matrix contains 3 rows and 8 columns as below.\n",
    "![DTM](https://i.imgur.com/8tk9vQH.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to determine the value(content) in the above matrix. I will discuss some of the ways below. After filling those values, we can use each row as vector representation of documents. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words\n",
    "In this, we will fill with the number of times that word occurred in the same document. \n",
    "\n",
    "<br>\n",
    "\n",
    "![BOW](https://i.imgur.com/lxNuXWh.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "If you check the above matrix, \"`nlp`\" occurred two times in the document-2 so value corresponding to that is 2.  If it occurs `n times` in the document, the value corresponding is `n`. We can do the same in the using `CountVectorizer` in  `sklearn`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words --> ['awesome', 'basic', 'easy', 'is', 'nlp', 'notebook', 'the', 'this']\n",
      "BOW Matrix --> [[0 0 0 1 1 1 1 1]\n",
      " [0 1 1 2 2 0 0 1]\n",
      " [1 0 0 1 1 0 0 0]]\n",
      "vocab to index dict --> {'this': 7, 'is': 3, 'the': 6, 'nlp': 4, 'notebook': 5, 'basic': 1, 'easy': 2, 'awesome': 0}\n"
     ]
    }
   ],
   "source": [
    "##import count vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#creating CountVectorizer instance\n",
    "bow_vec = CountVectorizer(lowercase=True, ngram_range=(1,1), analyzer='word')\n",
    "#fitting with our data\n",
    "bow_vec.fit(sample_documents)\n",
    "#transforming the data to the vector\n",
    "sample_bow_metrix = bow_vec.transform(sample_documents)\n",
    "#printing\n",
    "print(\"Unique words -->\", bow_vec.get_feature_names())\n",
    "print(\"BOW Matrix -->\",sample_bow_metrix.toarray())\n",
    "print(\"vocab to index dict -->\", bow_vec.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: How CountVectorizer gets the unique words? -- It first splits the documents into words and then it gets the unique words. `CountVectorizer` uses `token_pattern` or `tokenizer`, we can give our custom `tokenization` algorithm to get words from a sentence. Please try to read the documentation of the `sklearn` to know more about it.\n",
    "\n",
    "<br>\n",
    "\n",
    "We can also get the n-gram words as vocab. please check the below code. That was written for unigrams and bi-grams. \n",
    "\n",
    ">Note: N-grams are simply all combinations of adjacent words of length n that you can find in your source text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words --> ['awesome', 'basic', 'basic nlp', 'easy', 'is', 'is awesome', 'is basic', 'is easy', 'is the', 'nlp', 'nlp is', 'nlp nlp', 'nlp notebook', 'notebook', 'the', 'the nlp', 'this', 'this is']\n",
      "BOW Matrix --> [[0 0 0 0 1 0 0 0 1 1 0 0 1 1 1 1 1 1]\n",
      " [0 1 1 1 2 0 1 1 0 2 1 1 0 0 0 0 1 1]\n",
      " [1 0 0 0 1 1 0 0 0 1 1 0 0 0 0 0 0 0]]\n",
      "vocab to index dict --> {'this': 16, 'is': 4, 'the': 14, 'nlp': 9, 'notebook': 13, 'this is': 17, 'is the': 8, 'the nlp': 15, 'nlp notebook': 12, 'basic': 1, 'easy': 3, 'is basic': 6, 'basic nlp': 2, 'nlp nlp': 11, 'nlp is': 10, 'is easy': 7, 'awesome': 0, 'is awesome': 5}\n"
     ]
    }
   ],
   "source": [
    "#creating CountVectorizer instance with ngram_range = (1,2) i.e uni-gram and bi-gram\n",
    "bow_vec = CountVectorizer(lowercase=True, ngram_range=(1,2), analyzer='word')\n",
    "#fitting with our data\n",
    "bow_vec.fit(sample_documents)\n",
    "#transforming the data to the vector\n",
    "sample_bow_metrix = bow_vec.transform(sample_documents)\n",
    "#printing\n",
    "print(\"Unique words -->\", bow_vec.get_feature_names())\n",
    "print(\"BOW Matrix -->\",sample_bow_metrix.toarray())\n",
    "print(\"vocab to index dict -->\", bow_vec.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "In this, we will fill with `TF*IDF`. \n",
    "#### Term Frequency\n",
    "\\begin{align}\n",
    "TF_K = \\frac{\\text{No of times word K occurred in that document}}{\\text{Total number of words in that document}}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Note: `TF` of a word is only dependent on a particular document. It won't depend on the total corpus of documents. `TF` value of word changes from document to document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inverse Document Frequency\n",
    "\\begin{align}\n",
    "IDF_K = log(\\frac{\\text{Total number of documents}}{\\text{Number of documents with word K}} )\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Note: `IDF` of a word dependent on total corpus of documents. `IDF` value of word is constant for total corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can think `IDF` as the information content of the word.  \n",
    "\n",
    "\\begin{align}\n",
    "\\text{Information Content} = -log(\\text{Probability of Word}) \\\\\n",
    "\\\\\n",
    "\\text{Probability of Word K} = \\frac{\\text{Number of documents with word K}}{\\text{Total number of documents}}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate the `TFIDF` vectors using `TfidfVectorizer` in sklearn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words --> ['awesome', 'basic', 'easy', 'is', 'nlp', 'notebook', 'the', 'this']\n",
      "TFIDF Matrix --> \n",
      " [[0.         0.         0.         0.32630952 0.32630952 0.55249005\n",
      "  0.55249005 0.42018292]\n",
      " [0.         0.43157129 0.43157129 0.50978591 0.50978591 0.\n",
      "  0.         0.32822109]\n",
      " [0.76749457 0.         0.         0.45329466 0.45329466 0.\n",
      "  0.         0.        ]]\n",
      "vocab to index dict --> {'this': 7, 'is': 3, 'the': 6, 'nlp': 4, 'notebook': 5, 'basic': 1, 'easy': 2, 'awesome': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#creating TfidfVectorizer instance\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "#fitting with our data\n",
    "tfidf_vec.fit(sample_documents)\n",
    "#transforming the data to the vector\n",
    "sample_tfidf_metrix = tfidf_vec.transform(sample_documents)\n",
    "#printing\n",
    "print(\"Unique words -->\", tfidf_vec.get_feature_names())\n",
    "print(\"TFIDF Matrix -->\", '\\n',sample_tfidf_metrix.toarray())\n",
    "print(\"vocab to index dict -->\", tfidf_vec.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `TfidfVectorizer` also we can get the n-grams and we can give our own `tokenization` algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What if we have so much vocab in our corpus? \n",
    "\n",
    "If we have many unique words, our `BOW/TFIDF` vectors will be very high dimensional that may cause `curse of dimensionality` problem.  We can solve this with the below methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limiting the number of vocab in BOW/TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `CountVectorize`, we can do this using max_features, min_df, max_df. You can use vocabulary parameter to get specific words only. Try to read the documentation of `CountVectorize` to know better about those. You can check the sample code below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words --> ['awesome', 'is', 'nlp', 'this']\n",
      "BOW Matrix --> [[0 1 1 1]\n",
      " [0 2 2 1]\n",
      " [1 1 1 0]]\n",
      "vocab to index dict --> {'this': 3, 'is': 1, 'nlp': 2, 'awesome': 0}\n"
     ]
    }
   ],
   "source": [
    "#creating CountVectorizer instance, limited to 4 features only\n",
    "bow_vec = CountVectorizer(lowercase=True, ngram_range=(1,1),\n",
    "                                analyzer='word', max_features=4)\n",
    "#fitting with our data\n",
    "bow_vec.fit(sample_documents)\n",
    "#transforming the data to the vector\n",
    "sample_bow_metrix = bow_vec.transform(sample_documents)\n",
    "#printing\n",
    "print(\"Unique words -->\", bow_vec.get_feature_names())\n",
    "print(\"BOW Matrix -->\",sample_bow_metrix.toarray())\n",
    "print(\"vocab to index dict -->\", bow_vec.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can do similar thing with `TfidfVectorizer` with same parameters. Please read the documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some of the problems with the CountVectorizer and TfidfVectorizer\n",
    "- If we have a large corpus, vocabulary will also be large and for `fit` function, you have to get all documents into RAM. This may be impossible if you don't have sufficient RAM.\n",
    "- building the `vocab` requires a full pass over the dataset hence it is not possible to fit text classifiers in a strictly online manner.\n",
    "- After the `fit`, we have to store the `vocab dict`, which takes so much memory. If we want to deploy in `memory-constrained` environments like amazon lambda, IoT devices, mobile devices, etc.., these maybe not useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Important: We can solve the first problem with an iterator over the total data and building the `vocab` then, using that `vocab`, we can create the `BOW` matrix in the sparse format and then `TFIDF` vectors using `TfidfTransformer`. The sparse matrix won't take much space so, we can store the BOW sparse matrix in our RAM to create the TFIDF sparse matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have written a sample code to do that for the same data. I have iterated over the data,  created vocab, and using that vocab, created BOW.  We can write a much more optimized version of the code, This is just a sample to show. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words --> ['awesome', 'basic', 'easy', 'is', 'nlp', 'notebook', 'the', 'this']\n",
      "BOW Matrix --> [[0 0 0 1 1 1 1 1]\n",
      " [0 1 1 2 2 0 0 1]\n",
      " [1 0 0 1 1 0 0 0]]\n",
      "vocab to index dict --> {'awesome': 0, 'basic': 1, 'easy': 2, 'is': 3, 'nlp': 4, 'notebook': 5, 'the': 6, 'this': 7}\n"
     ]
    }
   ],
   "source": [
    "##for tokenization\n",
    "import nltk\n",
    "#vertical stack of sparse matrix\n",
    "from scipy.sparse import vstack\n",
    "#vocab set\n",
    "vocab_set = set()\n",
    "#looping through the points(for huge data, you will get from your disk/table)\n",
    "for data_point in sample_documents:\n",
    "    #getting words\n",
    "    for word in nltk.tokenize.word_tokenize(data_point):\n",
    "        if word.isalpha():\n",
    "            vocab_set.add(word.lower())\n",
    "\n",
    "vectorizer_bow = CountVectorizer(vocabulary=vocab_set)\n",
    "\n",
    "bow_data = [] \n",
    "for data_point in sample_documents: # use a generator\n",
    "    ##if we give the vocab, there will be no data lekage for fit_transform so we can use that\n",
    "    bow_data.append(vectorizer_bow.fit_transform([data_point]))\n",
    "\n",
    "final_bow = vstack(bow_data)\n",
    "\n",
    "print(\"Unique words -->\", vectorizer_bow.get_feature_names())\n",
    "print(\"BOW Matrix -->\",final_bow.toarray())\n",
    "print(\"vocab to index dict -->\", vectorizer_bow.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above result is similar to the one we printed while doing the BOW, you can check that.\n",
    "\n",
    "<br>\n",
    "\n",
    "Using the above `BOW` sparse matrix and the `TfidfTransformer`, we can create the `TFIDF` vectors. you can check below code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.32630952 0.32630952 0.55249005\n",
      "  0.55249005 0.42018292]\n",
      " [0.         0.43157129 0.43157129 0.50978591 0.50978591 0.\n",
      "  0.         0.32822109]\n",
      " [0.76749457 0.         0.         0.45329466 0.45329466 0.\n",
      "  0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#importing\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "#instanciate the class\n",
    "vec_tfidftransformer = TfidfTransformer()\n",
    "#fit with the BOW sparse data \n",
    "vec_tfidftransformer.fit(final_bow)\n",
    "vec_tfidf = vec_tfidftransformer.transform(final_bow)\n",
    "print(vec_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above result is similar to the one we printed while doing the TFIDF, you can check that.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Important: Other than our own iterator/generator, if we have data in one file or multiple files, we can directly give `input` parameter as `file/filename` and while `fit` function, we can give file path. Please read the documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to solve all the above problems are <b>hashing</b>. We can convert a word into fixed index number using the hash function. so, there will be no training process to get the vocabulary and no need to save the vocab. It was implemented in sklearn with `HashingVectorizer`. In `HashingVectorizer`, you have to mention number of features you need, by default it takes  $2^{20}$. below you can see some code to use `HashingVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hash vectors --> [[0. 1. 3. 1. 0.]\n",
      " [0. 1. 5. 1. 0.]\n",
      " [0. 0. 3. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#importing the hashvectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "#instanciating the HashingVectorizer\n",
    "hash_vectorizer = HashingVectorizer(n_features=5, norm=None, alternate_sign=False)\n",
    "#transforming the data, No need to fit the data because, it is stateless\n",
    "hash_vector = hash_vectorizer.transform(sample_documents)\n",
    "#printing the output\n",
    "print(\"Hash vectors -->\",hash_vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Note: You can normalize your vectors using norm. Since the hash function might cause collisions between (unrelated) features, a signed hash function is used and the sign of the hash value determines the sign of the value stored in the output matrix for a feature. This way, collisions are likely to cancel out rather than accumulate error, and the expected mean of any output feature’s value is zero. This mechanism is enabled by default with `alternate_sign=True` and is particularly useful for small hash table sizes (`n_features < 10000`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert above vector to `TFIDF` using `TfidfTransformer`. check the below code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf using hash BOW --> [[0.         0.36691832 0.85483442 0.36691832 0.        ]\n",
      " [0.         0.2419863  0.93961974 0.2419863  0.        ]\n",
      " [0.         0.         1.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#instanciate the class\n",
    "vec_idftrans = TfidfTransformer()\n",
    "#fit with the hash BOW sparse data \n",
    "vec_idftrans.fit(hash_vector)\n",
    "##transforming the data\n",
    "vec_tfidf2 = vec_idftrans.transform(hash_vector)\n",
    "print(\"tfidf using hash BOW -->\",vec_tfidf2.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This vectorizer is memory efficient but there are some cons for this as well, some of them are\n",
    "- There is no way to compute the inverse transform of the Hashing so there will be no interpretability of the model. \n",
    "- There can be collisions in the hashing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "1. https://scikit-learn.org/stable/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
