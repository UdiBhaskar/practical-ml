{
  
    
        "post0": {
            "title": "Foundations to inference Statistics",
            "content": "import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns from matplotlib import rcParams # figure size in inches rcParams[&#39;figure.figsize&#39;] = 11.7,8.27 . Sampling Distribution . . Steps for the sampling distribution . Get n random samples each of size m | Calculate the sample statistic for each random sample. | plot the sample statistics distribution | np.random.seed(10) random_population = np.random.beta(4, 5, size=1000) random_sample_population = np.random.choice(random_population, 400, False) # sns.distplot(random_sample_population, hist=True, kde=True, color=&#39;red&#39;, norm_hist=True, label=&quot;Population Dist&quot;) plt.title(&#39;Population&#39;) plt.legend() . &lt;matplotlib.legend.Legend at 0x2596b4cf608&gt; . print(&#39;random population Mean --&gt;&#39;, np.mean(random_sample_population)) print(&#39;random population SD --&gt;&#39;, np.std(random_sample_population)) . random population Mean --&gt; 0.4597295869346082 random population SD --&gt; 0.16002188241803594 . Code to get sampling distribution . no_of_samples = 5000 sample_size = 50 def get_sampling_dist(population, no_of_samples, sample_size, stat): &#39;&#39;&#39; population - random sample population we have - array or list no_of_samples - number of samples (n) sample_size - size of each sample.(m) stat - sample stat to calculate. - Function &#39;&#39;&#39; sampling_dist = [] for i in range(no_of_samples): ## n samples sample = np.random.choice(population, sample_size, False) # each of size m stat_val = stat(sample) #calculating stat sampling_dist.append(stat_val) return sampling_dist sampling_dist_mean = get_sampling_dist(random_sample_population, no_of_samples, sample_size, np.mean) . ##ploting the sampling distribution. sns.distplot(sampling_dist_mean, hist=True, kde=True, color=&#39;red&#39;, norm_hist=True, label=&quot;Sampling dist of mean&quot;) plt.title(&#39;Sampling Dist of Mean&#39;) plt.xlabel(&#39;sample means&#39;) plt.legend() . &lt;matplotlib.legend.Legend at 0x2596b4dea48&gt; . print(&#39;Sampling Dist Mean --&gt;&#39;, np.mean(sampling_dist_mean)) print(&#39;Sampling Dist SD --&gt;&#39;, np.std(sampling_dist_mean)) . Sampling Dist Mean --&gt; 0.46022309784271165 Sampling Dist SD --&gt; 0.020887127365585376 . We can do the above sampling distribution for any statistic value. | If we observe the above means, sampling dist mean(x_bar) is nearly equal to the population mean(mu) | We will call std of sampling dist as standard error | . Central Limit Theorem: . The distribution of sample means is nearly normal with mean = poluation mean, std = population_std/sqrt(sample_size). . . begin{align} text{Sampling Dist ~} N( mu, frac{ sigma}{ sqrt{ text{sample size}}}) end{align} . . Conditions . Sampled observations must be independent. | If we do sampling with replacement, the sample size must be less than 10% of the population. | The sample size of 30 is sufficient for most distributions. However, strongly skewed distributions can require larger sample sizes. | . Tip: We can simulate the CLT with https://gallery.shinyapps.io/CLT_mean/ . print(0.020887127365585376*np.sqrt(sample_size)) print(np.std(random_population)) . 0.14769429399712528 0.15723732430046308 . . Warning: Can the Central Limit theorem apply to any other sample statistic like median, std? -- No . Need for Confidence Interval . There will be variability in the point estimate because we can&#39;t get the exact population data in real-time. so if we tell a range of mean(any stat), it will be useful. This is called a Confidence Interval. Before going into CI, we have to know Z and t distribution, confidence level. . Z- Distribution or standard normal distribution: . begin{align} Z = frac{ x - mu}{ sigma} end{align} . . Note: mean = 0, std = 1 . standard_normal = np.random.standard_normal(size=100000) sns.distplot(standard_normal, hist=True, kde=True, color=&#39;red&#39;, norm_hist=True) plt.title(&#39;Z-Distribution&#39;) . Text(0.5, 1.0, &#39;Z-Distribution&#39;) . Confidence level . The probability that the value of a parameter falls within a specified range of values. This will be very useful when we tell an interval. We can tell like, with 95% confidence, our statistic/parameter lies between lower bound and upper bound. . . Let&#39;s calculate confidence levels for the above z distribution. For 95% confidence, (100-95)/2 = 2.5, we have to get the 2.5 percentile and (100-2.5)=97.5 percentile. . np.percentile(standard_normal, [2.5, 97.5]) . array([-1.94948191, 1.94622294]) . Our z scores lie between -1.95 and 1.95 with 95% confidence. based on above simulation( right value is 1.96, if we take more size, we will get 1.96) We can get this using (100-C)/2, 100-((100-c)/2) where C = 95 if we need 95% of confidence. . We can get these using the scipy.stats.norm.ppf function but in this function takes all the values with 0-1 only not 0-100 so 95% will become 0.95. We can get this using (1-C)/2, 1-((1-c)/2) where C = 0.95 if we need 95% of confidence. . from scipy.stats import norm def get_qnorm(CL): &#39;&#39;&#39;get the value in zdist for given CL CL - Confidence level(0-1)&#39;&#39;&#39; val = (1-CL)/2 return norm.ppf(val), norm.ppf(1-val) get_qnorm(0.95) . (-1.959963984540054, 1.959963984540054) . ##99 confidence level get_qnorm(0.99) . (-2.5758293035489004, 2.5758293035489004) . ##64 confidence level get_qnorm(0.684) . (-1.0027116650265495, 1.0027116650265495) . Student&#39;s t-distribution: . It is useful when population std is unknown. If the sample size is small, we may not get better results with the Z distribution. It is similar to the Z distribution bell-shaped curve but thicker tails than normal. Other than mean, std, it has another parameter called degree of freedom = n-1. It is wider so intervals that we get from t-dist are also wider. . . from scipy.stats import t def get_qnorm_t(CL, df): &#39;&#39;&#39;get the value in t-dist for given CL CL - Confidence level(0-1)&#39;&#39;&#39; val = (1-CL)/2 return t.ppf(val, df), t.ppf(1-val, df) get_qnorm_t(0.95, 29) . (-2.045229642132703, 2.045229642132703) . Difference between t-dist and z-dist . If we have more degrees of freedom(more samples), t-distribution will look like z-distribution. You can check that below. . for i in range(0, 1000, 50): print(&#39;95% of CL with df &#39;+str(i)+&#39; is --&gt;&#39;,get_qnorm_t(0.95, i)) . 95% of CL with df 0 is --&gt; (nan, nan) 95% of CL with df 50 is --&gt; (-2.008559109715206, 2.008559109715206) 95% of CL with df 100 is --&gt; (-1.9839715184496334, 1.9839715184496334) 95% of CL with df 150 is --&gt; (-1.9759053308869137, 1.9759053308869137) 95% of CL with df 200 is --&gt; (-1.9718962236316089, 1.9718962236316089) 95% of CL with df 250 is --&gt; (-1.9694983934204002, 1.9694983934204002) 95% of CL with df 300 is --&gt; (-1.9679030112607843, 1.9679030112607843) 95% of CL with df 350 is --&gt; (-1.9667650028635124, 1.9667650028635124) 95% of CL with df 400 is --&gt; (-1.965912343229391, 1.965912343229391) 95% of CL with df 450 is --&gt; (-1.965249664736427, 1.965249664736427) 95% of CL with df 500 is --&gt; (-1.9647198374673438, 1.9647198374673438) 95% of CL with df 550 is --&gt; (-1.964286550912067, 1.964286550912067) 95% of CL with df 600 is --&gt; (-1.9639256220427195, 1.9639256220427195) 95% of CL with df 650 is --&gt; (-1.963620322372358, 1.963620322372358) 95% of CL with df 700 is --&gt; (-1.963358711099814, 1.963358711099814) 95% of CL with df 750 is --&gt; (-1.9631320366857694, 1.9631320366857694) 95% of CL with df 800 is --&gt; (-1.9629337387277888, 1.9629337387277888) 95% of CL with df 850 is --&gt; (-1.9627588026071148, 1.9627588026071148) 95% of CL with df 900 is --&gt; (-1.9626033295371894, 1.9626033295371894) 95% of CL with df 950 is --&gt; (-1.962464242556152, 1.962464242556152) . get_qnorm(0.95) . (-1.959963984540054, 1.959963984540054) . get_qnorm_t(0.95, 50) . for i in range(0, 1000, 50): print(&#39;90% of CL with df &#39;+str(i)+&#39; is --&gt;&#39;,get_qnorm_t(0.90, i)) print(&#39;-&#39;*50) print(&#39;90% of CL in Z-dist is --&gt;&#39;, get_qnorm(0.90)) . 90% of CL with df 0 is --&gt; (nan, nan) 90% of CL with df 50 is --&gt; (-1.6759050245283318, 1.6759050245283311) 90% of CL with df 100 is --&gt; (-1.6602343260657506, 1.66023432606575) 90% of CL with df 150 is --&gt; (-1.655075500184607, 1.6550755001846063) 90% of CL with df 200 is --&gt; (-1.6525081009102696, 1.652508100910269) 90% of CL with df 250 is --&gt; (-1.6509714898126593, 1.6509714898126586) 90% of CL with df 300 is --&gt; (-1.6499486739375542, 1.6499486739375535) 90% of CL with df 350 is --&gt; (-1.6492188695371959, 1.6492188695371952) 90% of CL with df 400 is --&gt; (-1.6486719414653956, 1.648671941465395) 90% of CL with df 450 is --&gt; (-1.6482468047587875, 1.6482468047587868) 90% of CL with df 500 is --&gt; (-1.6479068539295052, 1.6479068539295045) 90% of CL with df 550 is --&gt; (-1.6476288171096811, 1.6476288171096805) 90% of CL with df 600 is --&gt; (-1.647397191759995, 1.6473971917599943) 90% of CL with df 650 is --&gt; (-1.6472012521875499, 1.6472012521875492) 90% of CL with df 700 is --&gt; (-1.6470333412605698, 1.647033341260569) 90% of CL with df 750 is --&gt; (-1.6468878462849894, 1.6468878462849887) 90% of CL with df 800 is --&gt; (-1.6467605593740848, 1.6467605593740842) 90% of CL with df 850 is --&gt; (-1.6466482638172075, 1.6466482638172069) 90% of CL with df 900 is --&gt; (-1.6465484584682117, 1.646548458468211) 90% of CL with df 950 is --&gt; (-1.6464591692544057, 1.646459169254405) -- 90% of CL in Z-dist is --&gt; (-1.6448536269514729, 1.6448536269514722) . From above, we can observe that, if df is large value( i.e n is large), t-distribution will yield similar results as z distribution. . Confidence Interval for Population Mean: . Why we need CI: There will be variability in the point estimate because we can&#39;t get the exact population data in real-time. so if we tell a range of mean, it will be useful. . . From the above CLT theorem, we know that sampling means follows a Normal distribution. we also know the properties of standard normal distribution like 68-95-99.7 rules( even we can compute for any value) i.e we can tell with 68% confidence that, mean is between mean-1*std_sampling_dist, mean+1*std_sampling_dist. . . begin{align} text{CI of Mean =} mu pm z^{*} * SE text{CI of Mean =} mu pm t^{*} * SE end{align} . from scipy.stats import norm def get_ci_mean(sampling_mean, SE, ci): &#39;&#39;&#39;Get CI for mean using z-dist sampling_mean - sample mean SE - Standard error from CLT CI - Confidence level&#39;&#39;&#39; z_temp = (1-(ci/100))/2 z = abs(norm.ppf(z_temp)) lower_bound = sampling_mean - z * SE upper_bound = sampling_mean + z * SE return lower_bound, upper_bound . n5_ci = get_ci_mean(np.mean(sampling_dist_mean), np.std(sampling_dist_mean), 95) print(&#39;95% CI is&#39;, n5_ci) . 95% CI is (0.4192850804656633, 0.5011611152197599) . from scipy.stats import t def get_ci_mean_t(sampling_mean, SE, ci, df): &#39;&#39;&#39;Get CI for mean using t-dist sampling_mean - sample mean SE - Standard error from CLT CI - Confidence level df - degrees of freedom, (n-1)&#39;&#39;&#39; t_temp = (1-(ci/100))/2 t_val = abs(t.ppf(t_temp, df)) lower_bound = sampling_mean - t_val * SE upper_bound = sampling_mean + t_val * SE return lower_bound, upper_bound . n5_ci_t = get_ci_mean_t(np.mean(sampling_dist_mean), np.std(sampling_dist_mean), 95, sample_size-1) print(&#39;95% CI is using t&#39;, n5_ci_t) . 95% CI is using t (0.41824884396920947, 0.5021973517162138) . len(sampling_dist_mean) . 5000 . . Tip: t distribution CI is wider than the Z distribution. Many times we don&#8217;t know what is the std of the population so it is better to use t distribution than z. If the sample size is larger, we can go for Z distribution(no issues if we have a large sample to analysis). . How to predict CI parameters other than mean . We know about sampling mean distribution so we can CI of mean very easily but how can we predict for median or percentile or IQR? . . There is another set of methods to do stats called Non-Parametric methods. We can use the non-parametric methods and get the CI for any value without knowing the underlying distribution. . Bootstrapping: . Bootstrap Sample is Sampling with replacement of data of same size as shown below . . Why same size? . the variation of the statistic will depend on the size of the sample. If we want to approximate this variation we need to use resamples of the same size. . Using Python . We can use np.random.choice to get the bootstrap samples. . ###code to generate a Bootstrap sample temp_population = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) print(&#39;Random Bootstrap sample from temp_population --&gt;&#39;, np.random.choice(temp_population, size=len(temp_population),replace=True)) print(&#39;Random Bootstrap sample from temp_population --&gt;&#39;, np.random.choice(temp_population, size=len(temp_population),replace=True)) print(&#39;Random Bootstrap sample from temp_population --&gt;&#39;, np.random.choice(temp_population, size=len(temp_population),replace=True)) . Random Bootstrap sample from temp_population --&gt; [ 3 10 1 8 5 8 8 3 6 5] Random Bootstrap sample from temp_population --&gt; [3 8 5 4 4 1 9 9 3 6] Random Bootstrap sample from temp_population --&gt; [10 10 4 7 2 10 9 2 7 3] . CI using Bootstrapping: . There are many ways to calculate bootstrap samples. . Percentile Method | Basic bootstrap Method or Reverse Percentile Interval | Studentized-t Bootstrap Method | Bias Corrected and accelerated | . Some notations: $ tilde{ theta}_{m}$ - statistic of Bootstrap sample m $ hat{ theta}$ - statistic of data we have. $ theta$ - statistic what we want to estimate. . Percentile Method: . steps: . Get the m bootstrap samples. | calculate the m statistics $ tilde{ theta}_{1}, tilde{ theta}_{1}, tilde{ theta}_{3} ... tilde{ theta}_{m}$ | Get the percentile values based in the how much confidance we need. Eg. for 95% CL, get 2.5 percentile and 97.5 percentile. | Code . np.random.seed(10) def get_percentile_bs_ci(data, m, stat, cl): &#39;&#39;&#39; percentile method to get BS-CI data - data we have(sample) m - number of bootstrap samples stat - statistic to find - a function cl - confidence level &#39;&#39;&#39; theta_stat = [] for i in range(m): bs_sample = np.random.choice(data, m) theta_stat.append(stat(bs_sample)) sns.distplot(theta_stat, hist=True, kde=True, color=&#39;red&#39;, norm_hist=True) lower_bound = (1-cl)/2 upper_bound = 1-lower_bound return np.percentile(theta_stat, [lower_bound*100, upper_bound*100]) . ##using above function to get bootstrap CI get_percentile_bs_ci(random_sample_population, 10000, np.mean, 0.95) . array([0.45656996, 0.46282595]) . We can ge the same thing from arch module . from arch.bootstrap import IIDBootstrap bs = IIDBootstrap(random_sample_population) . bs.conf_int(func=np.mean, reps=10000, method=&#39;percentile&#39;, size=0.95) . array([[0.44397996], [0.47526665]]) . . Important: CI from the percentile method is very narrow. If the underlying distribution is skew, it won&#8217;t work properly. so go for basic method or studentized-t method. . Basic Bootstrap Method or Reverse Percentile Interval Method: . steps: . calculate the statistic $ hat{ theta}$ on data we have. | Get the m bootstrap samples. | calculate the m statistics $ tilde{ theta}_{1}, tilde{ theta}_{1}, tilde{ theta}_{3} ... tilde{ theta}_{m}$ | Calculate the CI with above formula. . Note: It assumes distribution of $ hat{ theta}- tilde{ theta}$ and $ theta- hat{ theta}$ are simialr&lt;/b&gt; | Code . np.random.seed(10) def get_basic_bs_ci(data, m, stat, cl): &#39;&#39;&#39; Reverse Percentile Interval Method data- sample we have m - number of bootstrap samples stat - stat function to calculate cl - confidence level &#39;&#39;&#39; hat_theta = stat(data) theta_stat = [] for i in range(m): bs_sample = np.random.choice(data, m) theta_stat.append(stat(bs_sample)) #sns.distplot(theta_stat, hist=True, kde=True, color=&#39;red&#39;, norm_hist=True) lower_bound = (1-cl)/2 upper_bound = 1-lower_bound lower_bound1 = 2*hat_theta - np.percentile(theta_stat, upper_bound*100) upper_bound1 = 2*hat_theta - np.percentile(theta_stat, lower_bound*100) return lower_bound1, upper_bound1 . get_basic_bs_ci(random_sample_population, 1000, np.mean, 0.95) . (0.44972018316026047, 0.47056730107433303) . You can get the more optimized code from arch.bootstrap . from arch.bootstrap import IIDBootstrap bs = IIDBootstrap(random_sample_population) . bs.conf_int(func=np.mean, reps=10000, method=&#39;basic&#39;, size=0.95) . array([[0.44406911], [0.47499762]]) . Studentized-t Bootstrap Method: . If the distributions of $ hat{ theta}- tilde{ theta}$ and $ theta- hat{ theta}$ are not close, then the basic bootstrap confidence interval can be inaccurate. But even in this case, the distributions of $ frac{ hat{ theta}- tilde{ theta}}{SE( tilde{ theta})}$ and $ frac{ theta- hat{ theta}}{SE({ hat{ theta}})}$ could be close. hence we could use studentized bootstrap CI. . . . steps: . calculate the statistic $ hat{ theta}$ on data we have. | Get the m bootstrap samples. | For each Bootstrap sample compute the $ tilde{ theta}$ | compute $SE({ tilde{ theta}})$ | compute $q = frac{ tilde{ theta}- hat{ theta}}{SE( tilde{ theta})}$ | | Estimate $SE( hat{ theta})$ (You can directly compute or use another Bootstrap approach.) | Calculate CI using above formulation. | code . from arch.bootstrap import IIDBootstrap bs = IIDBootstrap(random_sample_population) . bs.conf_int(func=np.mean, reps=1000, method=&#39;studentized&#39;, size=0.95) . array([[0.44364941], [0.47571528]]) . Bias Corrected and accelerated bootstrap CI: . The main advantage to the BCa interval is that it corrects for bias and skewness in the distribution of bootstrap estimates. The BCa interval requires that you estimate two parameters. The bias-correction parameter, z0, is related to the proportion of bootstrap estimates that are less than the observed statistic. The acceleration parameter, a, is proportional to the skewness of the bootstrap distribution . To compute a BCa confidence interval, you estimate z0 and a and use them to adjust the endpoints of the percentile confidence interval (CI). If the bootstrap distribution is positively skewed, the CI is adjusted to the right. If the bootstrap distribution is negatively skewed, the CI is adjusted to the left. . If the statistic is biased upward (that is, if it tends to be too large), the BCa bias correction moves the endpoints to the left. If the bootstrap distribution is skewed to the right, the BCa incorporates a correction to move the endpoints even farther to the right. . You can read more about calculating z0 and a in https://projecteuclid.org/download/pdf_1/euclid.ss/1032280214 . code . from arch.bootstrap import IIDBootstrap bs = IIDBootstrap(random_sample_population) . bs.conf_int(func=np.mean, reps=1000, method=&#39;bca&#39;, size=0.95) . array([[0.44443402], [0.47456652]]) . When to use what? . If we have small sample size, basic methods like &quot;basic&quot;, &quot;percentile&quot; may give wider intervels so &#39;BCa&#39; or &#39;Studentized-t&#39; may be better. If you have skewness, go for &#39;BCa&#39;. | If we have large data, all methods may give better intervals but if we have skewness, it is better to go for &#39;BCa&#39;. | . . . REF 2 &quot;&gt;https://en.wikipedia.org/wiki/Bootstrapping_(statistics)}}&lt;/a&gt; &quot;&gt;https://christofseiler.github.io/stats205/}}&lt;/a&gt; &quot;&gt;https://www.coursera.org/learn/inferential-statistics-intro/}}&lt;/a&gt; .",
            "url": "https://udibhaskar.github.io/practical-ml/clt/sampling/z-distribution/t-distribution/ci/confidence%20interval/bootstrapping/2020/06/07/Foundations-to-inference-Stats.html",
            "relUrl": "/clt/sampling/z-distribution/t-distribution/ci/confidence%20interval/bootstrapping/2020/06/07/Foundations-to-inference-Stats.html",
            "date": " • Jun 7, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Probability Distributions and use cases",
            "content": "Discrete distributions . import numpy as np import matplotlib.pyplot as plt import seaborn as sns from matplotlib import rcParams # figure size in inches rcParams[&#39;figure.figsize&#39;] = 11.7,8.27 . Bernoulli distribution . A Bernoulli random variable has exactly two possible outcomes. We typically label one of these outcomes a &quot;success&quot; and the other outcome a &quot;failure&quot;. We may also denote a success by 1 and a failure by 0. . The flip of a coin is modeled well by a Bernoulli distribution, as it is a single trial with a fixed nonzero probability of success (even if that probability is difficult to pin down if the coin is unfair). . Note: Probability of success = p, mean = p, Var = p(1-p) . We can silumate using np.random.random() . import numpy as np np.random.random(1) . Binomial Distribution . The binomial distribution describes a sequence of identical, independent Bernoulli trials. That is, each trial has the same probability of success, and the results of one trial do not affect any of the following trials. . let Probability of success = p . begin{align} text{Probability of k success in n trails} = P(k) &amp;= binom{n}{k} p^k (1-p)^{n-k} end{align} . Note: mean = np, Var = np(1-p) . . From above formual, we can tell given a 20 coin flips, what is the probability of getting 7 heads if getting a head is having probability of 0.35. n = 20 k = 7 p = 0.35 Then, P(7) = 0.18440118638 . We can simulate using np.random.binomial . n = 20 p = 0.35 size=1000 np.random.seed(4) vals = np.random.binomial(n, p, size) sns.distplot(vals, kde=False, rug=False, color=&#39;red&#39;, label=&quot;number of heads in simulation of 20 coin flips&quot;) plt.xlabel(&#39;No of Heads&#39;) plt.ylabel(&#39;No of times occur out of 1000 times&#39;) plt.legend() . &lt;matplotlib.legend.Legend at 0x2da2f8bfa88&gt; . sns.distplot(vals, hist=False, kde=True, color=&#39;red&#39;, label=&quot;Probability of number of heads in simulation of 20 coin flips&quot;) plt.xlabel(&#39;No of Heads&#39;) plt.ylabel(&#39;Probability&#39;) plt.legend() . &lt;matplotlib.legend.Legend at 0x2da301f0608&gt; . count_7 = sum(vals==7) print(&quot;count&quot;, count_7) prob = count_7/1000 print(&#39;probability&#39;, prob) . count 179 probability 0.179 . Theoritical probability we got from formula is 0.1844 and now we got 0.179. . From above simulation we can also tell for probability if every any number of heads. we got a curve which is similar to normal distribution. . . Warning: To check binomial distribution, It has to satisfy below conditions - The trials are independent. - The number of trials is fixed(n). - Each trial outcome is classified as a success or failure. - The probability of success(p) is the same for each trial. . Usecase-1 . Let&#39;s take you are working in a food delivery company. Based on previous deliveries, the probability of delivering the wrong item is 0.0085. Per day, company delivers average of 1500 items. Loss per one wrong delivery is 100rs. Calculate the maximum loss we may get? | based on above info n = 1500, p = 0.0085 . np.random.seed(5) wrong_deliveries = np.random.binomial(1500, 0.0085, 10000) . sns.distplot(wrong_deliveries, hist=False, kde=True, color=&#39;red&#39;, label=&quot;Wrong Deliveries&quot;) plt.hist(wrong_deliveries, density=True) plt.xlabel(&#39;No of deliveries&#39;) plt.ylabel(&#39;Probability&#39;) plt.legend() . &lt;matplotlib.legend.Legend at 0x2da2fb3ef08&gt; . max(wrong_deliveries) . 30 . kwargs = {&#39;cumulative&#39;: True} sns.distplot(wrong_deliveries, hist_kws=kwargs, kde_kws=kwargs, label=&#39;CDF of # of wrong delivery&#39;, color=&#39;red&#39;) plt.legend() . &lt;matplotlib.legend.Legend at 0x2da350e6d08&gt; . np.percentile(wrong_deliveries, 95) . 19.0 . Based on CDF or using np.percentile we can tell how many wrong orders with a probability and the we can calculate the loss. . Usecase-2 . Let&#39;s take you are working in a Manufacturing company. Based on previous data, 3% of items produced are defective. If we produce 5000 items a day, what is probabability to get a 4500 non defective items. or analyze the how many days it may needed to get 5 million non defective items? | Usecase-3 . Let&#39;s consider you are working in a tele-marketing company. Probability of converting a lead to sale is 6.5%, No of lead calls is 100 per day.. If you want to increase the revenue by some 10%, how many calls we have to make or how much conversion rate we need? How many employees we can recruit to increase the no of calls? | Poisson Distribution . The Poisson distribution is often useful for estimating the number of events in a large population over a unit of time. . Let&#39;s say, we have to calculate how many hits we will get to my website hourly. let&#39;s say probability of hit is 0.2 per hour. . We can model this even using the binomial RV if we have one hit or not in a hour but here we may get zero hits, 1 hit or some hour may give more than 1 hit. The problem with binomial is that it cannot contain more than 1 event in the unit of time (in this case, 1 hr is the unit time). The unit of time can only have 0 or 1 event. If we divid the unit of time into smaller parts, we can handle it i.e if we do n --&gt; infinite ( i.e. p --&gt; 0) in the Binomial distribution, we can model it. . The Poisson Distribution, doesn’t require you to know n or p. We are assuming n is infinitely large and p is infinitesimal. The only parameter of the Poisson distribution is the rate λ (the expected value of x). . begin{align} P(k) = frac{e^{- lambda} lambda^{k}} {k!} end{align} . Note: mean = Lambda, Var = Lambda . Usecase-1 . Let&#39;s take you are working in E-Learning company. Based on previous data, on average comapany getting 7 queries per hour. What is the probability that getting 12 queries or more in the next hour. | lamda = 7 np.random.seed(12) no_queries = np.random.poisson(lamda, size=20000) . sns.distplot(no_queries, hist=False, kde=True, color=&#39;red&#39;, label=&quot;no of queries&quot;) plt.hist(no_queries, density=True) plt.xlabel(&#39;No of queries&#39;) plt.ylabel(&#39;Probability&#39;) plt.legend() . &lt;matplotlib.legend.Legend at 0x2da359304c8&gt; . print(&#39;probability&#39;, sum(no_queries&gt;=12)/len(no_queries)) . probability 0.0545 . Geometric Distribution . The geometric distribution represents the number of failures before the first success in a sequence of Bernoulli trials. . . let Probability of success = p . . begin{align} text{Probability of having exactly k failures before the first success} = P(k) = (1-p)^{k}p end{align} . . We can simulate it using np.random.geometric . Use Case-1 . A programmer has a 90% chance of finding a bug every time he compiles his code, and it takes him two hours to rewrite his code every time he discovers a bug. What is the probability that he will finish his program by the end of his workday? Assume that a workday is 8 hours and that the programmer compiles his code immediately at the beginning of the day. | Use Case-2 . 2.. In cost-benefit analyses, such as a company deciding whether to fund research trials that, if successful, will earn the company some estimated profit, the goal is to reach a success before the cost outweighs the potential gain. . Continuous distributions . Normal distribution . The normal distribution is described by two parameters μ and σ, representing the mean and standard deviation of the random variable X respectively. . . begin{align} text{Probability Density Fn} = f(x) &amp;= dfrac{1}{ sqrt{2 pi} sigma} e^{- dfrac{(x- mu)^2}{2 sigma^2}} end{align} . . We can simulate it using np.random.normal . normal_vals = np.random.normal(0, 5, 1000) . sns.distplot(normal_vals, hist=False, kde=True, color=&#39;red&#39;, label=&quot;Normal Dist&quot;) plt.hist(normal_vals, density=True) plt.legend() . &lt;matplotlib.legend.Legend at 0x2da35d863c8&gt; . . 68-95-99.7 rule . . Exponential distribution . To predict the amount of waiting time until the next event (i.e., success, failure, arrival, etc.). You can thik similar to poission distribution. In poission distribution, we can predict how many events occur in a unit of time. In Exponential distribution, amout of time needed to event occur. If the number of events per unit time follows a Poisson distribution, then the amount of time between events follows the exponential distribution. . . begin{align} text{Probability Density Fn} = f(x) &amp;= lambda e^{- lambda k} end{align} . . Warning: X ~ Exp(0.3) is to remember that 0.3 is not a time duration, but it is an event rate, which is the same as the parameter λ in a Poisson process. When rate changes, it won&#8217;t work. rate has to be constant . . We can simulate it using np.random.exponential . . Use case-1 . At a call center, calls come in every 20 minutes on average. What is the approximate probability that no calls will come in for a 30 minute period? | The time between calls can be represented by an exponential distribution with Lambda=3, since one call every 20 minutes is 3 calls per hour. we have to get the probability that at least half an hour passes between calls . np.random.seed(6) calls_time = np.random.exponential(1/3, 10000) . sum(vals&gt;0.5)/len(vals) . 0.2292 . sns.distplot(calls_time, hist=False, kde=True, color=&#39;red&#39;, label=&quot;Calls time&quot;) plt.hist(calls_time, density=True) plt.legend() . &lt;matplotlib.legend.Legend at 0x2da35d98648&gt; . Use case-2 . A computer repair customer service takes an average of 3 days repair. How long (approximately, in days) should customer has to wait to pickup repaired product with 95% confidance? | The repair time can be represented by an exponential distribution with λ=1/3, the number of repairs per day. . np.random.seed(8) manf_time = np.random.exponential(3, 10000) . manf_time . array([ 6.20086514, 10.37717835, 6.1021323 , ..., 5.30842985, 1.94993868, 3.36773582]) . n5p_value = np.percentile(manf_time, 95) n5p_value . 8.912912006371394 . The customer should wait atleast 9 days to be 95% sure that the repair has done. . Log Normal . it describes a random variable whose logarithm is normally distributed. .",
            "url": "https://udibhaskar.github.io/practical-ml/probability/distribution/bernoulli/binomial/normal/poission/geometric/lognormal/2020/06/05/Distributions.html",
            "relUrl": "/probability/distribution/bernoulli/binomial/normal/poission/geometric/lognormal/2020/06/05/Distributions.html",
            "date": " • Jun 5, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am a Machine Learning Engineer with four years of industry experience. Currently working on Query resolution systems and semantic similarity search with NLP. Involve in research and designing an approach, followed by code development. You can check my Linkedin and GitHub .",
          "url": "https://udibhaskar.github.io/practical-ml/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://udibhaskar.github.io/practical-ml/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}